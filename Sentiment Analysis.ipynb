{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "import tweepy as tp\n",
    "import pandas as pd\n",
    "import re\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "# !!IMPORTANT!!: You need to install the package \"mysqlclient\", so some functions of sqlalchemy can work.\n",
    "\n",
    "# Personal Codes\n",
    "AccessToken = ''\n",
    "AccessTokenSecret = ''\n",
    "APIKey = ''\n",
    "APIKeySecret = ''\n",
    "\n",
    "# Tweepy functions to authenticate our developer's account / app.\n",
    "auth = tp.OAuthHandler(APIKey, APIKeySecret)\n",
    "auth.set_access_token(AccessToken, AccessTokenSecret)\n",
    "api = tp.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Connect to MySQL and create a database, using 'mysql.connector'\n",
    "db = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"\")\n",
    "mycursor = db.cursor()\n",
    "mycursor.execute(\"CREATE DATABASE IF NOT EXISTS sentimentdb\")\n",
    "# Close MySQL connection, using 'mysql.connector'\n",
    "db.close()\n",
    "\n",
    "# Connect to MySQL so we can insert later a dataframe as it is, using sqlalchemy\n",
    "engine = create_engine('mysql://root:@localhost/sentimentdb?charset=utf8', encoding='utf-8')\n",
    "\n",
    "\n",
    "# Now the same procedure will be followed for the 3 categories: Tesla, Toyota, and BMW.\n",
    "# Hashtag comments will only be used for the first category as the procedure is the same for the other 2.\n",
    "\n",
    "##############\n",
    "#  1. Tesla  #\n",
    "##############\n",
    "\n",
    "# Create empty list to fill later, through iterations\n",
    "tweets = []\n",
    "i = 1\n",
    "print(\"1. Tesla\")\n",
    "# Api.search looks for tweets based on the info we are feeding it.\n",
    "# We include tweets that contain the: \"#Tesla\".\n",
    "# We exclude: Retweets, Languages other than english.\n",
    "# .items(n) is the number if iterations. It will extract 'n' number of tweets.\n",
    "for tweet in tp.Cursor(api.search,\n",
    "                       q=\"#Tesla\",\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode=\"extended\",\n",
    "                       include_rts=0).items(30):\n",
    "\n",
    "    # We iterate and print i, just to see the progress of our extraction\n",
    "    print(i)\n",
    "    i += 1\n",
    "\n",
    "    # We create a list with the information asked in this order:\n",
    "    # User name, Number of followers, Number of tweets, Number of tweet's retweets, Text, Date, Location, Hashtags\n",
    "    try:\n",
    "        data = [tweet.user.screen_name,\n",
    "                tweet.user.followers_count,\n",
    "                tweet.user.statuses_count,\n",
    "                tweet.retweet_count,\n",
    "                re.sub(r\"(@|http)\\S+\", \"\", tweet.full_text),\n",
    "                tweet.created_at,\n",
    "                tweet.place,\n",
    "                tweet.entities['hashtags']]\n",
    "\n",
    "        # Transform list into tuple\n",
    "        data = tuple(data)\n",
    "        # Append it to the \"tweets\" list\n",
    "        tweets.append(data)\n",
    "\n",
    "    # Exclude error: \"Sorry, that page does not exist\"\n",
    "    except tp.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "\n",
    "    # Break iteration\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "# Transform list to a pandas Data Frame\n",
    "tesla = pd.DataFrame(tweets,\n",
    "                     columns=['User', 'Followers_U', 'Tweets_U', 'Retweets', 'Text', 'Date', 'Location', 'Hashtags'])\n",
    "\n",
    "# Create a CSV out of the Data Frame\n",
    "tesla.to_csv(path_or_buf='C:/Users/User/Desktop/1.Tesla.csv')\n",
    "\n",
    "# Insert dataframe to MySQL table 'Tesla'.\n",
    "tesla = pd.read_csv('C:/Users/User/Desktop/1.Tesla.csv', encoding='utf-8')\n",
    "tesla.to_sql('tesla', con=engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "###############\n",
    "#  2. Toyota  #\n",
    "###############\n",
    "\n",
    "tweets = []\n",
    "i = 1\n",
    "print(\"\")\n",
    "print(\"2. Toyota\")\n",
    "for tweet in tp.Cursor(api.search,\n",
    "                       q=\"#Toyota\",\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode=\"extended\",\n",
    "                       include_rts=0).items(30):\n",
    "    print(i)\n",
    "    i += 1\n",
    "\n",
    "    try:\n",
    "        data = [tweet.user.screen_name,\n",
    "                tweet.user.followers_count,\n",
    "                tweet.user.statuses_count,\n",
    "                tweet.retweet_count,\n",
    "                re.sub(r\"(@|http)\\S+\", \"\", tweet.full_text),\n",
    "                tweet.created_at,\n",
    "                tweet.place,\n",
    "                tweet.entities['hashtags']]\n",
    "\n",
    "        data = tuple(data)\n",
    "        tweets.append(data)\n",
    "\n",
    "    except tp.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "toyota = pd.DataFrame(tweets,\n",
    "                      columns=['User', 'Followers_U', 'Tweets_U', 'Retweets', 'Text', 'Date', 'Location', 'Hashtags'])\n",
    "toyota.to_csv(path_or_buf='C:/Users/User/Desktop/2.Toyota.csv')\n",
    "toyota = pd.read_csv('C:/Users/User/Desktop/2.Toyota.csv', encoding='utf-8')\n",
    "toyota.to_sql('toyota', con=engine, if_exists='append', index=False)\n",
    "\n",
    "############\n",
    "#  3. BMW  #\n",
    "############\n",
    "\n",
    "tweets = []\n",
    "i = 1\n",
    "print(\"\")\n",
    "print(\"3. BMW\")\n",
    "for tweet in tp.Cursor(api.search,\n",
    "                       q=\"#BMW\",\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode=\"extended\",\n",
    "                       include_rts=0).items(30):\n",
    "    print(i)\n",
    "    i += 1\n",
    "\n",
    "    try:\n",
    "        data = [tweet.user.screen_name,\n",
    "                tweet.user.followers_count,\n",
    "                tweet.user.statuses_count,\n",
    "                tweet.retweet_count,\n",
    "                re.sub(r\"(@|http)\\S+\", \"\", tweet.full_text),\n",
    "                tweet.created_at,\n",
    "                tweet.place,\n",
    "                tweet.entities['hashtags']]\n",
    "\n",
    "        data = tuple(data)\n",
    "        tweets.append(data)\n",
    "\n",
    "    except tp.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "bmw = pd.DataFrame(tweets,\n",
    "                   columns=['User', 'Followers_U', 'Tweets_U', 'Retweets', 'Text', 'Date', 'Location', 'Hashtags'])\n",
    "bmw.to_csv(path_or_buf='C:/Users/User/Desktop/3.BMW.csv')\n",
    "bmw = pd.read_csv('C:/Users/User/Desktop/3.BMW.csv', encoding='utf-8')\n",
    "bmw.to_sql('bmw', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Close MySQL connection, using sqlalchemy\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from textblob import TextBlob\n",
    "from sqlalchemy import create_engine\n",
    "# !!IMPORTANT!!: You need to install the package \"mysqlclient\", so some functions of sqlalchemy can work.\n",
    "\n",
    "\n",
    "# We read the CSVs\n",
    "tesla = pd.read_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/1.Tesla.csv\", encoding='utf-8')\n",
    "toyota = pd.read_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/2.Toyota.csv\", encoding='utf-8')\n",
    "bmw = pd.read_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/3.BMW.csv\", encoding='utf-8')\n",
    "\n",
    "# We add through iteration, the sentiment polarity for each text, as an extra column in our dataframes.\n",
    "tesla['Polarity'] = tesla.apply(lambda x: TextBlob(x['Text']).sentiment.polarity, axis=1)\n",
    "toyota['Polarity'] = toyota.apply(lambda x: TextBlob(x['Text']).sentiment.polarity, axis=1)\n",
    "bmw['Polarity'] = bmw.apply(lambda x: TextBlob(x['Text']).sentiment.polarity, axis=1)\n",
    "\n",
    "# We create a column named \"Sentiment\" which classifies text based on their Polarity.\n",
    "#  1 = Positive\n",
    "# -1 = Negative\n",
    "#  0 = Neutral\n",
    "tesla['Sentiment'] = np.nan\n",
    "toyota['Sentiment'] = np.nan\n",
    "bmw['Sentiment'] = np.nan\n",
    "\n",
    "for i in range(0, 420):\n",
    "    if tesla.iloc[i, 9] > 0:\n",
    "        tesla.iloc[i, 10] = 1\n",
    "    elif tesla.iloc[i, 9] < 0:\n",
    "        tesla.iloc[i, 10] = -1\n",
    "    else:\n",
    "        tesla.iloc[i, 10] = 0\n",
    "\n",
    "for i in range(0, 420):\n",
    "    if toyota.iloc[i, 9] > 0:\n",
    "        toyota.iloc[i, 10] = 1\n",
    "    elif toyota.iloc[i, 9] < 0:\n",
    "        toyota.iloc[i, 10] = -1\n",
    "    else:\n",
    "        toyota.iloc[i, 10] = 0\n",
    "\n",
    "for i in range(0, 420):\n",
    "    if bmw.iloc[i, 9] > 0:\n",
    "        bmw.iloc[i, 10] = 1\n",
    "    elif bmw.iloc[i, 9] < 0:\n",
    "        bmw.iloc[i, 10] = -1\n",
    "    else:\n",
    "        bmw.iloc[i, 10] = 0\n",
    "\n",
    "# Firstly, the first column which is unnamed, we named it 'Id' using the notebook application. Then:\n",
    "# We fix the 'Id' column. It is the python's indexing. So This column has values 0-29 for every day.\n",
    "# We change it to 1 to 420 for each table\n",
    "for i in range(0, 420):\n",
    "    tesla.iloc[i, 0] = i+1\n",
    "    toyota.iloc[i, 0] = i+1\n",
    "    bmw.iloc[i, 0] = i+1\n",
    "\n",
    "# We replace the existing raw dataframes in the MySQL database, with the dataframes with the extra sentiment columns.\n",
    "# Same procedure as described in \"1. Scraping\", but instead of 'append' we use 'replace'.\n",
    "db = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"\")\n",
    "mycursor = db.cursor()\n",
    "mycursor.execute(\"CREATE DATABASE IF NOT EXISTS sentimentdb\")\n",
    "db.close()\n",
    "engine = create_engine('mysql://root:@localhost/sentimentdb?charset=utf8', encoding='utf-8')\n",
    "tesla.to_sql('tesla', con=engine, if_exists='replace', index=False)\n",
    "toyota.to_sql('toyota', con=engine, if_exists='replace', index=False)\n",
    "bmw.to_sql('bmw', con=engine, if_exists='replace', index=False)\n",
    "engine.dispose()\n",
    "\n",
    "# We export the final CSVs\n",
    "tesla.to_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/1.Tesla F.csv\", index=False)\n",
    "toyota.to_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/2.Toyota F.csv\", index=False)\n",
    "bmw.to_csv(\"D:/1. MASTER/5. Programming for Data Science/2. Homework/3.BMW F.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
